import json
import logging
import re
import hashlib
from typing import Dict, List, Any
from .openrouter_client import OpenRouterClient
from .sor_analysis_engine import SORAnalysisEngine

logger = logging.getLogger(__name__)

class VulnerabilityAnalyzer:
    def __init__(self):
        self.openrouter_client = OpenRouterClient()
        self.sor_engine = SORAnalysisEngine()
        self.last_analysis_time = 0
        self.last_cost = 0.0
        self.sor_enhanced_count = 0
        # Cache for duplicate analysis prevention
        self.analysis_cache = {}
        self.cache_hits = 0
        self.cache_misses = 0
    
    SYSTEM_PROMPT = """
You are vuln-chaser, a Pattern-Free Interactive Application Security Testing (IAST) tool that performs creative, unrestricted vulnerability analysis.

YOUR CORE PRINCIPLES:
- **Pattern-Free Analysis**: You are NOT limited by predefined vulnerability patterns or categories
- **Creative Security Thinking**: Think beyond traditional OWASP categories and discover novel attack vectors
- **Runtime-Based Discovery**: Analyze actual execution traces to identify unexpected security risks
- **Adaptive Analysis**: Let the execution context guide your analysis, not predetermined rules

YOUR UNIQUE CAPABILITIES:
- Discover unknown and emerging vulnerability patterns
- Identify complex, multi-step attack chains
- Detect business logic vulnerabilities that pattern-based tools miss
- Find subtle trust boundary violations and privilege escalation paths
- Recognize novel data flow vulnerabilities and side-channel attacks

ANALYTICAL APPROACH:
- Examine source code + execution context without bias toward specific vulnerability types
- Think like a creative attacker: "How could this execution be misused?"
- Consider unconventional attack vectors and edge cases
- Identify vulnerabilities that combine multiple seemingly harmless operations
- Focus on "what could go wrong" rather than "what matches known patterns"

Always provide innovative, context-driven security analysis based on actual runtime execution patterns.
"""
    
    async def analyze_batch(self, trace_batch: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Analyze a batch of traces for vulnerabilities with SOR Framework integration"""
        try:
            # First, perform SOR analysis on the entire batch
            logger.info("ðŸ” Starting SOR Framework analysis...")
            sor_results = await self.sor_engine.analyze_trace_batch(trace_batch)
            
            # Then, analyze each trace with SOR context for vulnerabilities
            results = []
            for sor_result in sor_results:
                try:
                    if sor_result.get('error'):
                        # Handle SOR analysis errors
                        results.append({
                            "trace_id": sor_result.get("trace_id", "unknown"),
                            "vulnerabilities": [],
                            "sor_analysis": None,
                            "error": f"SOR analysis failed: {sor_result['error']}"
                        })
                        continue
                    
                    # Analyze with SOR context
                    result = await self.analyze_with_sor_context(sor_result)
                    results.append(result)
                    
                except Exception as e:
                    logger.error(f"Failed to analyze trace {sor_result.get('trace_id', 'unknown')}: {e}")
                    results.append({
                        "trace_id": sor_result.get("trace_id", "unknown"),
                        "vulnerabilities": [],
                        "sor_analysis": sor_result.get('sor_analysis'),
                        "error": str(e)
                    })
            
            return results
            
        except Exception as e:
            logger.error(f"Failed to analyze trace batch: {e}")
            return []
    
    
    async def analyze_with_sor_context(self, sor_result: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze a trace with SOR Framework context for enhanced vulnerability detection"""
        trace_id = sor_result.get('trace_id', 'unknown')
        sor_analysis = sor_result.get('sor_analysis')
        
        if not sor_analysis:
            logger.error(f"No SOR analysis available for trace {trace_id}")
            return {
                "trace_id": trace_id,
                "vulnerabilities": [],
                "error": "SOR analysis not available"
            }
        
        # Check cache before expensive LLM analysis
        execution_trace = sor_result.get('execution_trace', [])
        trace_hash = self._compute_trace_hash(execution_trace)
        
        if trace_hash in self.analysis_cache:
            self.cache_hits += 1
            cached_result = self.analysis_cache[trace_hash].copy()
            cached_result['trace_id'] = trace_id  # Update trace ID
            cached_result['cache_hit'] = True
            logger.info(f"ðŸš€ Cache hit for trace {trace_id} (hash: {trace_hash[:8]}...)")
            logger.info(f"ðŸ“Š Cache stats: {self.cache_hits} hits, {self.cache_misses} misses")
            return cached_result
        
        self.cache_misses += 1
        logger.info(f"ðŸ” Cache miss for trace {trace_id} (hash: {trace_hash[:8]}...)")
        logger.info(f"ðŸ“Š Cache stats: {self.cache_hits} hits, {self.cache_misses} misses")
        
        # Build SOR-enhanced prompt
        prompt = self._build_sor_enhanced_prompt(sor_result)
        
        # Debug: Log analysis start
        logger.debug(f"Starting SOR-enhanced vulnerability analysis for trace {trace_id}")
        
        messages = [
            {"role": "system", "content": self.SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ]
        
        response = await self.openrouter_client.chat_completion(messages)
        
        # Debug: Log completion
        logger.debug(f"LLM analysis completed for trace {trace_id}")
        
        # Update metrics
        self.last_analysis_time = response["response_time_ms"]
        self.last_cost = response["cost_usd"]
        self.sor_enhanced_count += 1
        
        # Parse LLM response
        vulnerabilities = self._parse_vulnerability_response(response["content"])
        
        # Enhance vulnerabilities with SOR context
        enhanced_vulnerabilities = self._enhance_vulnerabilities_with_sor(vulnerabilities, sor_analysis)
        
        result = {
            "trace_id": trace_id,
            "vulnerabilities": enhanced_vulnerabilities,
            "sor_analysis": sor_analysis,
            "cache_hit": False,
            "analysis_metadata": {
                "model": response["model"],
                "response_time_ms": response["response_time_ms"],
                "cost_usd": response["cost_usd"],
                "usage": response["usage"],
                "sor_enhanced": True,
                "sor_analysis_summary": sor_analysis.get('summary', {})
            }
        }
        
        # Cache the result for future use (without trace_id)
        cacheable_result = result.copy()
        cacheable_result['trace_id'] = None  # Remove trace_id for caching
        self.analysis_cache[trace_hash] = cacheable_result
        logger.info(f"ðŸ’¾ Cached analysis for hash {trace_hash[:8]}... (cache size: {len(self.analysis_cache)})")
        
        return result
    
    def _build_sor_enhanced_prompt(self, sor_result: Dict[str, Any]) -> str:
        """Build SOR Framework enhanced vulnerability detection prompt"""
        sor_analysis = sor_result.get('sor_analysis', {})
        execution_trace = sor_result.get('execution_trace', [])
        
        # Extract SOR components
        subject = sor_analysis.get('subject', {})
        operations = sor_analysis.get('operations', [])
        resources = sor_analysis.get('resources', [])
        relationships = sor_analysis.get('relationships', {})
        summary = sor_analysis.get('summary', {})
        
        # Build SOR context description
        sor_context = self._format_sor_context(subject, operations, resources, relationships, summary)
        
        # Format execution trace
        trace_context = self._format_execution_trace(execution_trace)
        
        # Debug logging
        logger.debug(f"Trace context length: {len(trace_context)}, SOR context available: {bool(sor_context)}")
        
        prompt = f"""vuln-chaser Pattern-Free Security Analysis

You are performing a creative, unrestricted security analysis of a Ruby application's runtime execution. This analysis is based on actual method call traces captured during execution, combined with SOR Framework context.

## PATTERN-FREE ANALYSIS MISSION

**THINK CREATIVELY** - You are NOT constrained by traditional vulnerability categories or patterns. Your job is to think like an innovative attacker and discover security risks that pattern-based tools would miss.

## Runtime Execution Context
{trace_context}

## SOR Framework Intelligence
{sor_context}

## CREATIVE ANALYSIS GUIDELINES

### 1. **Beyond Traditional Patterns**
- Don't just look for SQL injection, XSS, or command injection
- Think about business logic flaws and workflow manipulation
- Consider timing attacks, race conditions, and state corruption
- Identify information disclosure through side channels
- Look for privilege escalation through unexpected paths

### 2. **Multi-Step Attack Chains**
- How could an attacker combine multiple seemingly harmless operations?
- What happens if operations are executed in unexpected orders?
- Could legitimate functionality be chained to achieve malicious goals?

### 3. **Context-Aware Creativity**
- What assumptions does this code make about its environment?
- How could those assumptions be violated or exploited?
- What would happen if the execution context was different than expected?

### 4. **Novel Attack Vectors**
- What unconventional ways could this execution be misused?
- Are there any subtle trust model violations?
- Could this expose sensitive information through unexpected channels?

## Pattern-Free Output Format
{{
  "pattern_free_analysis": {{
    "creative_risk_assessment": {{
      "overall_risk_level": "low|medium|high|critical",
      "novel_risks_identified": "Description of unconventional security risks discovered",
      "attack_surface_analysis": "Creative assessment of exposed attack vectors",
      "business_logic_vulnerabilities": "Workflow and process manipulation risks"
    }},
    "innovative_findings": {{
      "multi_step_attack_chains": ["Sequences of operations that could be chained for attacks"],
      "trust_model_violations": ["Subtle violations of application trust assumptions"],
      "side_channel_risks": ["Information disclosure through timing, errors, or behavior"],
      "state_manipulation_opportunities": ["Ways to corrupt or manipulate application state"]
    }}
  }},
  "vulnerabilities": [
    {{
      "vulnerability_classification": "Create your own classification - don't limit to OWASP categories",
      "severity": "low|medium|high|critical", 
      "confidence": 0.0-1.0,
      "affected_component": "Specific method/component where issue exists",
      "creative_description": "Innovative explanation of the security risk discovered",
      "attack_scenario": "Step-by-step exploitation scenario",
      "business_impact": "Real-world consequences of this vulnerability",
      "novel_aspects": "What makes this vulnerability unique or hard to detect",
      "remediation_strategy": "Creative solutions to address the vulnerability",
      "sor_relationship": "How Subject-Operation-Resource relationships enable this risk"
    }}
  ]
}}

## CREATIVE ANALYSIS PRINCIPLES
- **Think Outside Categories**: Don't be constrained by traditional vulnerability types
- **Focus on Execution Reality**: Analyze what actually happened, not theoretical risks
- **Creative Attack Mindset**: Think like an innovative attacker discovering new techniques
- **Holistic Context**: Consider the full application context and environment
- **Novel Discovery**: Prioritize finding risks that pattern-based tools would miss

Provide your analysis in the specified JSON format."""
        return prompt
    
    def _format_execution_trace(self, execution_trace: List[Dict[str, Any]]) -> str:
        """Format execution trace for LLM analysis"""
        if not execution_trace:
            return "No execution trace available"
        
        formatted_lines = []
        for i, trace in enumerate(execution_trace[:100]):
            method = trace.get("method", "Unknown")
            file_location = trace.get("file", "unknown")
            line = trace.get("line", 0)
            source = trace.get("source", "").strip()
            context = trace.get("context", "")
            
            # Debug log for source code
            logger.debug(f"Trace {i}: method={method}, source_length={len(source)}")
            
            formatted_lines.append(f"""
{i+1}. **{method}** ({file_location}:{line})
   ```
   {source}
   ```
   Context: {context}
""")
        
        if len(execution_trace) > 15:
            formatted_lines.append(f"\n... and {len(execution_trace) - 15} more method calls")
        
        return "\n".join(formatted_lines)
    
    def _parse_vulnerability_response(self, response_content: str) -> List[Dict[str, Any]]:
        """Parse LLM response to extract vulnerabilities"""
        try:
            logger.debug(f"Processing LLM response, length: {len(response_content)}")
            
            # Try multiple strategies to extract JSON
            json_content = None
            
            # Strategy 1: Look for ```json code blocks
            if '```json' in response_content:
                start_marker = response_content.find('```json') + 7
                end_marker = response_content.find('```', start_marker)
                if end_marker != -1:
                    json_content = response_content[start_marker:end_marker].strip()
                    logger.debug("ðŸ“‹ Found JSON in code block")
                else:
                    # Handle truncated response in code block
                    json_content = response_content[start_marker:].strip()
                    logger.debug("ðŸ“‹ Found truncated JSON in code block")
            
            # Strategy 2: Look for JSON object boundaries
            if not json_content:
                start_idx = response_content.find('{')
                if start_idx != -1:
                    # Count braces to find matching closing brace
                    brace_count = 0
                    end_idx = start_idx
                    for i, char in enumerate(response_content[start_idx:], start_idx):
                        if char == '{':
                            brace_count += 1
                        elif char == '}':
                            brace_count -= 1
                            if brace_count == 0:
                                end_idx = i + 1
                                break
                    
                    if brace_count == 0:
                        json_content = response_content[start_idx:end_idx]
                        logger.debug("ðŸ“‹ Found JSON by brace matching")
                    else:
                        # Handle incomplete JSON - try to fix it
                        json_content = response_content[start_idx:]
                        logger.debug("ðŸ“‹ Found incomplete JSON, will attempt repair")
            
            # Strategy 3: Try to find just vulnerabilities array
            if not json_content:
                vuln_match = re.search(r'"vulnerabilities"\s*:\s*\[([^\]]*(?:\[[^\]]*\][^\]]*)*)\]', response_content, re.DOTALL)
                if vuln_match:
                    vuln_array = vuln_match.group(1).strip()
                    json_content = f'{{"vulnerabilities": [{vuln_array}]}}'
                    logger.debug("ðŸ“‹ Found vulnerabilities array only")
            
            if not json_content:
                logger.warning("âŒ No JSON content found in LLM response")
                return []
            
            logger.debug(f"Extracted JSON content, length: {len(json_content)}")
            
            # Try to parse directly first
            try:
                parsed = json.loads(json_content)
                logger.debug("âœ… Direct JSON parsing successful")
            except json.JSONDecodeError:
                logger.debug("ðŸ”§ Direct parsing failed, applying fixes...")
                # Apply fixes and try again
                json_content = self._fix_json_content(json_content)
                logger.debug(f"Applied JSON fixes, new length: {len(json_content)}")
                parsed = json.loads(json_content)
                logger.debug("âœ… Fixed JSON parsing successful")
            
            vulnerabilities = parsed.get("vulnerabilities", [])
            
            # Store vuln-chaser analysis for later use
            vuln_chaser_analysis = parsed.get("vuln_chaser_analysis", {})
            
            # Validate and clean up vulnerabilities
            valid_vulnerabilities = []
            for vuln in vulnerabilities:
                if self._validate_vulnerability(vuln):
                    # Add vuln-chaser analysis context to each vulnerability
                    vuln['vuln_chaser_context'] = vuln_chaser_analysis
                    valid_vulnerabilities.append(vuln)
                else:
                    logger.warning(f"Invalid vulnerability format: {vuln}")
            
            logger.info(f"âœ… Successfully parsed {len(valid_vulnerabilities)} vulnerabilities")
            return valid_vulnerabilities
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ Failed to parse JSON from LLM response: {e}")
            logger.error(f"ðŸ“‹ Raw response: {response_content}")
            if 'json_content' in locals():
                logger.error(f"ðŸ”§ Processed JSON: {json_content}")
            
            # Final fallback: return empty list
            logger.warning("ðŸ”„ Returning empty vulnerabilities list as fallback")
            return []
            
        except Exception as e:
            logger.error(f"âŒ Unexpected error parsing vulnerability response: {e}")
            return []
    
    def _validate_vulnerability(self, vuln: Dict[str, Any]) -> bool:
        """Validate Pattern-Free vulnerability structure"""
        # Pattern-Free required fields
        required_fields = ["vulnerability_classification", "severity", "confidence", "affected_component"]
        
        for field in required_fields:
            if field not in vuln:
                logger.warning(f"Missing required field: {field}")
                return False
        
        # Validate severity (same as before)
        valid_severities = ["low", "medium", "high", "critical"]
        if vuln["severity"] not in valid_severities:
            logger.warning(f"Invalid severity: {vuln['severity']}")
            return False
        
        # Validate confidence (same as before)
        try:
            confidence = float(vuln["confidence"])
            if not (0.0 <= confidence <= 1.0):
                logger.warning(f"Invalid confidence value: {confidence}")
                return False
        except (ValueError, TypeError):
            logger.warning(f"Invalid confidence type: {vuln['confidence']}")
            return False
        
        # Pattern-Free: Accept any vulnerability classification (no OWASP restriction)
        logger.debug(f"âœ… Pattern-Free vulnerability validated: {vuln['vulnerability_classification']}")
        return True
    
    def _fix_json_content(self, json_content: str) -> str:
        """Fix common JSON formatting issues in LLM responses, including truncated responses"""
        
        # Remove markdown code block markers
        json_content = json_content.replace('```json', '').replace('```', '')
        json_content = json_content.strip()
        
        # Handle truncated JSON by attempting to close incomplete structures
        if not json_content.endswith('}'):
            logger.debug("ðŸ”§ Attempting to repair truncated JSON")
            
            # Count opening vs closing braces to determine how many we need
            open_braces = json_content.count('{')
            open_brackets = json_content.count('[')
            
            # Find last complete section before truncation
            last_complete = max(
                json_content.rfind('}'),
                json_content.rfind(']'),
                json_content.rfind(',')
            )
            
            if last_complete > len(json_content) * 0.7:  # If we have most of the content
                # Truncate at last complete section and close properly
                json_content = json_content[:last_complete + 1]
                
                # Add missing closing braces/brackets
                missing_braces = open_braces - json_content.count('}')
                missing_brackets = open_brackets - json_content.count(']')
                
                json_content += ']' * missing_brackets
                json_content += '}' * missing_braces
                
                logger.debug(f"ðŸ”§ Repaired JSON by adding {missing_brackets} ] and {missing_braces} }}")
        
        # Fix trailing commas before closing braces/brackets  
        json_content = re.sub(r',(\s*[}\]])', r'\1', json_content)
        
        # Fix unescaped newlines and tabs in strings
        json_content = json_content.replace('\n', '\\n').replace('\t', '\\t')
        
        # Fix unescaped quotes in string values - more comprehensive approach
        # First, escape backslashes that aren't already escaped
        json_content = re.sub(r'(?<!\\)\\(?!["\\nt/])', r'\\\\', json_content)
        
        # Fix unescaped quotes in string values with a more robust approach
        def fix_quotes_in_strings(text):
            """Fix quotes within JSON string values"""
            result = []
            i = 0
            in_string = False
            escape_next = False
            
            while i < len(text):
                char = text[i]
                
                if escape_next:
                    result.append(char)
                    escape_next = False
                elif char == '\\':
                    result.append(char)
                    escape_next = True
                elif char == '"':
                    if not in_string:
                        # Start of string
                        in_string = True
                        result.append(char)
                    else:
                        # Check if this is the end of the string or a quote inside
                        # Look ahead to see if this might be the end of a JSON value
                        next_chars = text[i+1:i+10].strip()
                        if (next_chars.startswith(',') or 
                            next_chars.startswith('}') or 
                            next_chars.startswith(']') or
                            next_chars == '' or
                            next_chars.startswith('\n')):
                            # This looks like the end of the string
                            in_string = False
                            result.append(char)
                        else:
                            # This is likely a quote inside the string - escape it
                            result.append('\\"')
                else:
                    result.append(char)
                
                i += 1
            
            return ''.join(result)
        
        # Apply the quote fixing
        json_content = fix_quotes_in_strings(json_content)
        
        # Ensure property names are quoted
        json_content = re.sub(r'([{,]\s*)([a-zA-Z_][a-zA-Z0-9_]*)(\s*):', r'\1"\2"\3:', json_content)
        
        return json_content
    
    def _format_sor_context(self, subject: Dict[str, Any], operations: List[Dict[str, Any]], 
                           resources: List[Dict[str, Any]], relationships: Dict[str, Any], 
                           summary: Dict[str, Any]) -> str:
        """Format SOR analysis results for prompt inclusion"""
        
        context_parts = []
        
        # Subject Analysis
        context_parts.append("### Subject Analysis")
        context_parts.append(f"- Type: {subject.get('type', 'unknown')}")
        context_parts.append(f"- Authentication Status: {subject.get('authentication_status', 'unknown')}")
        context_parts.append(f"- Privilege Level: {subject.get('privilege_level', 'unknown')}")
        context_parts.append(f"- Trust Boundary: {subject.get('trust_boundary', 'unknown')}")
        
        # Operations Analysis
        if operations:
            context_parts.append("\n### Operations Analysis")
            for i, op in enumerate(operations[:5]):  # Limit to first 5 operations
                primary = op.get('primary_classification', {})
                context_parts.append(f"**Operation {i+1}**: {op.get('trace_method', 'unknown')}")
                context_parts.append(f"  - Category: {primary.get('category', 'unknown')}")
                context_parts.append(f"  - Risk Level: {primary.get('risk_level', 'unknown')}")
                context_parts.append(f"  - Implementation Quality: {primary.get('implementation_quality', 'unknown')}")
                vuln_patterns = op.get('vulnerability_patterns', [])
                if vuln_patterns:
                    context_parts.append(f"  - Vulnerability Patterns: {', '.join(vuln_patterns)}")
        
        # Resources Analysis
        if resources:
            context_parts.append("\n### Resources Analysis")
            for i, res in enumerate(resources[:5]):  # Limit to first 5 resources
                context_parts.append(f"**Resource {i+1}**: {res.get('trace_method', 'unknown')}")
                context_parts.append(f"  - Type: {res.get('type', 'unknown')}")
                context_parts.append(f"  - Sensitivity: {res.get('sensitivity', 'unknown')}")
                context_parts.append(f"  - Access Pattern: {res.get('access_pattern', 'unknown')}")
                protections = res.get('protection_mechanisms', [])
                context_parts.append(f"  - Protection Mechanisms: {', '.join(protections) if protections else 'None'}")
        
        # Relationship Analysis
        context_parts.append("\n### SOR Relationship Analysis")
        violations = relationships.get('violations', [])
        if violations:
            context_parts.append(f"**Detected Violations ({len(violations)} total):**")
            for violation in violations[:5]:  # Limit to first 5 violations
                v_type = violation.get('type', 'unknown')
                severity = violation.get('severity', 'unknown')
                desc = violation.get('description', '')
                context_parts.append(f"  - {v_type} (Severity: {severity}): {desc}")
        else:
            context_parts.append("**Detected Violations**: None")
        
        risk_score = relationships.get('risk_score', 0)
        context_parts.append(f"**Risk Score**: {risk_score}/10.0")
        
        recommendations = relationships.get('recommendations', [])
        if recommendations:
            context_parts.append(f"**Recommendations**: {', '.join(recommendations[:3])}")
        
        # Summary
        if summary:
            context_parts.append("\n### Analysis Summary")
            context_parts.append(f"- Overall Risk Level: {summary.get('risk_level', 'unknown')}")
            context_parts.append(f"- Total Violations: {summary.get('total_violations', 0)}")
            context_parts.append(f"- Key Finding: {summary.get('message', 'None')}")
        
        return "\n".join(context_parts)
    
    def _enhance_vulnerabilities_with_sor(self, vulnerabilities: List[Dict[str, Any]], 
                                         sor_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Enhance detected vulnerabilities with SOR context information"""
        
        enhanced_vulnerabilities = []
        relationships = sor_analysis.get('relationships', {})
        
        for vuln in vulnerabilities:
            # Create enhanced copy
            enhanced_vuln = vuln.copy()
            
            # Add SOR context enhancement flag
            enhanced_vuln['sor_context_enhanced'] = True
            
            # Add related SOR violations
            violations = relationships.get('violations', [])
            related_violations = []
            for violation in violations:
                # Check if violation is related to this vulnerability type
                if self._is_violation_related_to_vulnerability(violation, vuln):
                    related_violations.append({
                        'type': violation.get('type', 'unknown'),
                        'severity': violation.get('severity', 'unknown'),
                        'description': violation.get('description', '')
                    })
            
            enhanced_vuln['related_sor_violations'] = related_violations
            
            # Add risk amplification information based on SOR analysis
            risk_score = relationships.get('risk_score', 0)
            if risk_score >= 7.0:
                enhanced_vuln['sor_risk_amplification'] = "High risk confirmed by SOR analysis"
                # Increase severity if supported by high SOR risk
                if enhanced_vuln['severity'] in ['low', 'medium'] and related_violations:
                    enhanced_vuln['severity'] = 'high'
                    enhanced_vuln['severity_boost_reason'] = 'SOR violation support'
            elif risk_score >= 4.0:
                enhanced_vuln['sor_risk_amplification'] = "Medium risk confirmed by SOR analysis"
            else:
                enhanced_vuln['sor_risk_amplification'] = "Low risk per SOR analysis"
            
            # Adjust confidence based on SOR analysis support
            if related_violations:
                # Increase confidence if SOR violations support the vulnerability
                original_confidence = float(enhanced_vuln.get('confidence', 0.5))
                boost = min(0.3, len(related_violations) * 0.1)  # Max boost of 0.3
                enhanced_vuln['confidence'] = min(1.0, original_confidence + boost)
                enhanced_vuln['confidence_boost_reason'] = f"Supported by {len(related_violations)} SOR violation(s)"
            
            # Add SOR-based remediation context
            if 'recommendation' in enhanced_vuln:
                sor_recommendations = relationships.get('recommendations', [])
                if sor_recommendations:
                    enhanced_vuln['sor_remediation_context'] = f"SOR Analysis suggests: {', '.join(sor_recommendations[:2])}"
            
            enhanced_vulnerabilities.append(enhanced_vuln)
        
        return enhanced_vulnerabilities
    
    def _is_violation_related_to_vulnerability(self, violation: Dict[str, Any], 
                                             vulnerability: Dict[str, Any]) -> bool:
        """Check if a SOR violation is related to a detected vulnerability"""
        
        violation_type = violation.get('type', '').lower()
        vuln_type = vulnerability.get('type', '').lower()
        vuln_category = vulnerability.get('owasp_category', '')
        
        # Define mapping between SOR violations and vulnerability types/categories
        violation_mappings = {
            'trust_boundary_violation': {
                'vuln_types': ['injection', 'sql_injection', 'command_injection', 'xss', 'xxe'],
                'owasp_categories': ['A03:2021']  # Injection
            },
            'privilege_escalation': {
                'vuln_types': ['access_control', 'authorization', 'authentication', 'privilege'],
                'owasp_categories': ['A01:2021', 'A07:2021']  # Access Control, Authentication
            },
            'insufficient_validation': {
                'vuln_types': ['injection', 'sql_injection', 'xss', 'validation', 'sanitization'],
                'owasp_categories': ['A03:2021', 'A05:2021']  # Injection, Security Misconfiguration
            },
            'direct_access': {
                'vuln_types': ['access_control', 'authorization', 'path_traversal', 'file_access'],
                'owasp_categories': ['A01:2021', 'A05:2021']  # Access Control, Security Misconfiguration
            }
        }
        
        # Check if violation type maps to this vulnerability
        mapping = violation_mappings.get(violation_type, {})
        
        # Check vulnerability type matching
        related_types = mapping.get('vuln_types', [])
        if any(vtype in vuln_type for vtype in related_types):
            return True
        
        # Check OWASP category matching
        related_categories = mapping.get('owasp_categories', [])
        if vuln_category in related_categories:
            return True
        
        return False
    
    def _compute_trace_hash(self, execution_trace: List[Dict[str, Any]]) -> str:
        """Compute hash of execution trace for deduplication based on call chain pattern"""
        if not execution_trace:
            return "empty_trace"
        
        # Extract key components for hashing - focus on structural patterns, not specific values
        hash_components = []
        for trace in execution_trace:
            # Use method name and file path as primary identifiers
            method = trace.get('method', '')
            file_path = trace.get('file', '')
            
            # Extract security-relevant context patterns (not specific values)
            context = trace.get('context', '')
            risk_level = trace.get('risk_level', '')
            param_usage = trace.get('parameter_usage', {})
            
            # Create normalized patterns for security analysis
            security_pattern = []
            if param_usage.get('uses_request_params'):
                security_pattern.append('uses_params')
            if param_usage.get('direct_interpolation'):
                security_pattern.append('direct_interpolation')
            if param_usage.get('sanitization_detected'):
                security_pattern.append('sanitized')
            
            # Create structural signature (excluding variable content like line numbers, specific source)
            signature = f"{method}:{file_path}:{context}:{risk_level}:{':'.join(sorted(security_pattern))}"
            hash_components.append(signature)
        
        # Create a deterministic hash of the call chain pattern
        trace_pattern = "|".join(hash_components)
        return hashlib.sha256(trace_pattern.encode('utf-8')).hexdigest()
    
    def clear_analysis_cache(self):
        """Clear the analysis cache"""
        cache_size = len(self.analysis_cache)
        self.analysis_cache.clear()
        self.cache_hits = 0
        self.cache_misses = 0
        logger.info(f"ðŸ§¹ Cleared analysis cache ({cache_size} entries)")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """Get cache performance statistics"""
        total_requests = self.cache_hits + self.cache_misses
        hit_rate = (self.cache_hits / total_requests * 100) if total_requests > 0 else 0
        
        return {
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses,
            'total_requests': total_requests,
            'hit_rate_percent': round(hit_rate, 2),
            'cache_size': len(self.analysis_cache)
        }
    
    def get_sor_performance_metrics(self) -> Dict[str, Any]:
        """Get SOR Framework performance metrics"""
        base_metrics = {
            'total_sor_enhanced_analyses': self.sor_enhanced_count,
            'last_analysis_time_ms': self.last_analysis_time,
            'last_cost_usd': self.last_cost
        }
        
        # Add cache performance metrics
        cache_stats = self.get_cache_stats()
        base_metrics['cache_performance'] = cache_stats
        
        # Get SOR engine performance metrics if available
        if hasattr(self.sor_engine, 'get_performance_metrics'):
            sor_metrics = self.sor_engine.get_performance_metrics()
            base_metrics['sor_component_metrics'] = sor_metrics
        
        return base_metrics
