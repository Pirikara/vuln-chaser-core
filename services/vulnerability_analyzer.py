import json
import logging
import re
import hashlib
from typing import Dict, List, Any
from .openrouter_client import OpenRouterClient
from .sor_analysis_engine import SORAnalysisEngine

logger = logging.getLogger(__name__)

class VulnerabilityAnalyzer:
    def __init__(self):
        self.openrouter_client = OpenRouterClient()
        self.sor_engine = SORAnalysisEngine()
        self.last_analysis_time = 0
        self.last_cost = 0.0
        self.total_sor_time = 0
        self.sor_enhanced_count = 0
        # Cache for duplicate analysis prevention
        self.analysis_cache = {}
        self.cache_hits = 0
        self.cache_misses = 0
    
    SYSTEM_PROMPT = """
You are vuln-chaser, an Interactive Application Security Testing (IAST) tool specializing in runtime vulnerability analysis using the SOR Framework (Subject-Operation-Resource).

Your expertise includes:
- Runtime security trace analysis for Ruby applications (web apps, CLI tools, libraries, scripts)
- SOR Framework-based contextual vulnerability detection
- OWASP Top 10 2021 compliance verification
- Trust boundary violation detection
- Authentication bypass and privilege escalation analysis
- Data flow security validation across different application types

Always provide actionable, context-aware security recommendations based on actual runtime execution patterns, regardless of application type (web, CLI, library, or script).
"""
    
    async def analyze_batch(self, trace_batch: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Analyze a batch of traces for vulnerabilities with SOR Framework integration"""
        try:
            # First, perform SOR analysis on the entire batch
            logger.info("🔍 Starting SOR Framework analysis...")
            sor_results = await self.sor_engine.analyze_trace_batch(trace_batch)
            
            # Then, analyze each trace with SOR context for vulnerabilities
            results = []
            for sor_result in sor_results:
                try:
                    if sor_result.get('error'):
                        # Handle SOR analysis errors
                        results.append({
                            "trace_id": sor_result.get("trace_id", "unknown"),
                            "vulnerabilities": [],
                            "sor_analysis": None,
                            "error": f"SOR analysis failed: {sor_result['error']}"
                        })
                        continue
                    
                    # Analyze with SOR context
                    result = await self.analyze_with_sor_context(sor_result)
                    results.append(result)
                    
                except Exception as e:
                    logger.error(f"Failed to analyze trace {sor_result.get('trace_id', 'unknown')}: {e}")
                    results.append({
                        "trace_id": sor_result.get("trace_id", "unknown"),
                        "vulnerabilities": [],
                        "sor_analysis": sor_result.get('sor_analysis'),
                        "error": str(e)
                    })
            
            return results
            
        except Exception as e:
            logger.error(f"Failed to analyze trace batch: {e}")
            return []
    
    async def analyze_with_sor_context(self, sor_result: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze a trace with SOR Framework context for enhanced vulnerability detection"""
        trace_id = sor_result.get('trace_id', 'unknown')
        sor_analysis = sor_result.get('sor_analysis')
        
        if not sor_analysis:
            logger.error(f"No SOR analysis available for trace {trace_id}")
            return {
                "trace_id": trace_id,
                "vulnerabilities": [],
                "error": "SOR analysis not available"
            }
        
        # Check cache before expensive LLM analysis
        execution_trace = sor_result.get('execution_trace', [])
        trace_hash = self._compute_trace_hash(execution_trace)
        
        if trace_hash in self.analysis_cache:
            self.cache_hits += 1
            cached_result = self.analysis_cache[trace_hash].copy()
            cached_result['trace_id'] = trace_id  # Update trace ID
            cached_result['cache_hit'] = True
            logger.info(f"🚀 Cache hit for trace {trace_id} (hash: {trace_hash[:8]}...)")
            logger.info(f"📊 Cache stats: {self.cache_hits} hits, {self.cache_misses} misses")
            return cached_result
        
        self.cache_misses += 1
        logger.info(f"🔍 Cache miss for trace {trace_id} (hash: {trace_hash[:8]}...)")
        logger.info(f"📊 Cache stats: {self.cache_hits} hits, {self.cache_misses} misses")
        
        # Build SOR-enhanced prompt
        prompt = self._build_sor_enhanced_prompt(sor_result)
        
        # Debug: Log the SOR-enhanced prompt
        logger.info(f"🔍 SOR-ENHANCED VULNERABILITY ANALYSIS for trace {trace_id}:")
        logger.info("=" * 80)
        logger.info(prompt)
        logger.info("=" * 80)
        
        messages = [
            {"role": "system", "content": self.SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ]
        
        response = await self.openrouter_client.chat_completion(messages)
        
        # Debug: Log the LLM response
        logger.info(f"🤖 SOR-ENHANCED LLM RESPONSE for trace {trace_id}:")
        logger.info("=" * 80)
        logger.info(response["content"])
        logger.info("=" * 80)
        
        # Update metrics
        self.last_analysis_time = response["response_time_ms"]
        self.last_cost = response["cost_usd"]
        self.sor_enhanced_count += 1
        
        # Parse LLM response
        vulnerabilities = self._parse_vulnerability_response(response["content"])
        
        # Enhance vulnerabilities with SOR context
        enhanced_vulnerabilities = self._enhance_vulnerabilities_with_sor(vulnerabilities, sor_analysis)
        
        result = {
            "trace_id": trace_id,
            "vulnerabilities": enhanced_vulnerabilities,
            "sor_analysis": sor_analysis,
            "cache_hit": False,
            "analysis_metadata": {
                "model": response["model"],
                "response_time_ms": response["response_time_ms"],
                "cost_usd": response["cost_usd"],
                "usage": response["usage"],
                "sor_enhanced": True,
                "sor_analysis_summary": sor_analysis.get('summary', {})
            }
        }
        
        # Cache the result for future use (without trace_id)
        cacheable_result = result.copy()
        cacheable_result['trace_id'] = None  # Remove trace_id for caching
        self.analysis_cache[trace_hash] = cacheable_result
        logger.info(f"💾 Cached analysis for hash {trace_hash[:8]}... (cache size: {len(self.analysis_cache)})")
        
        return result
    
    def _build_sor_enhanced_prompt(self, sor_result: Dict[str, Any]) -> str:
        """Build SOR Framework enhanced vulnerability detection prompt"""
        sor_analysis = sor_result.get('sor_analysis', {})
        execution_trace = sor_result.get('execution_trace', [])
        
        # Extract SOR components
        subject = sor_analysis.get('subject', {})
        operations = sor_analysis.get('operations', [])
        resources = sor_analysis.get('resources', [])
        relationships = sor_analysis.get('relationships', {})
        summary = sor_analysis.get('summary', {})
        
        # Build SOR context description
        sor_context = self._format_sor_context(subject, operations, resources, relationships, summary)
        
        # Format execution trace
        trace_context = self._format_execution_trace(execution_trace)
        
        prompt = f"""vuln-chaser IAST Analysis Request

You are analyzing runtime execution traces from a Ruby application using vuln-chaser's SOR Framework. This is an Interactive Application Security Testing (IAST) analysis based on actual method call traces captured during runtime execution (web requests, CLI operations, lib functionality, or script execution).

## Runtime Execution Context
{trace_context}

## SOR Framework Intelligence
vuln-chaser has automatically analyzed the execution trace and identified the following SOR relationships:
{sor_context}

## vuln-chaser SOR Analysis Protocol

**Subject Analysis**: Who initiated the execution and their trust level
**Operation Analysis**: Security controls present/missing in the execution chain  
**Resource Analysis**: Sensitivity of accessed data/operations and protection mechanisms
**Relationship Analysis**: Violations of security policies between Subject-Operation-Resource chains

Focus on identifying where untrusted input reaches sensitive operations without proper security controls.

## Vulnerability Detection Focus
Apply OWASP Top 10 2021 standards with special attention to:
- **Runtime Data Flow**: How external input reaches sensitive operations
- **Trust Boundary Violations**: Untrusted subjects accessing protected resources
- **Missing Security Controls**: Gaps in validation, authentication, or authorization
- **Application Context**: Security requirements vary by application type (web/CLI/library/script)

## vuln-chaser Output Format
{{
  "vuln_chaser_analysis": {{
    "execution_summary": {{
      "subject_classification": "authenticated_user|anonymous_user|cli_user|lib_caller|script_invoker|service_account|external_system",
      "trust_level": "trusted|semi_trusted|untrusted",
      "execution_risk_level": "low|medium|high|critical",
      "application_type": "web_app|cli_tool|lib_library|script|api_service"
    }},
    "security_findings": {{
      "missing_controls": ["list of missing security controls"],
      "risky_operations": ["operations that pose security risks"],
      "exposed_resources": ["resources accessed without proper protection"]
    }}
  }},
  "vulnerabilities": [
    {{
      "owasp_category": "A01:2021|A02:2021|A03:2021|A05:2021|A06:2021|A07:2021",
      "type": "Descriptive vulnerability name",
      "severity": "low|medium|high|critical", 
      "confidence": 0.0-1.0,
      "affected_method": "Ruby method where vulnerability exists",
      "description": "Clear explanation of the vulnerability found in the execution trace",
      "exploitation_scenario": "How an attacker could exploit this vulnerability",
      "recommendation": "Specific code changes needed to fix this vulnerability",
      "sor_context": "Which Subject-Operation-Resource relationship enables this vulnerability"
    }}
  ]
}}

## Analysis Guidelines
- **Runtime-First**: Analyze actual execution paths, not theoretical vulnerabilities
- **SOR-Driven**: Use the provided SOR relationships to understand security context  
- **Practical Impact**: Only report exploitable vulnerabilities with clear attack paths
- **Context-Aware**: Consider application type (web/CLI/library/script) in recommendations

Provide your analysis in the specified JSON format."""
        return prompt
    
    def _format_execution_trace(self, execution_trace: List[Dict[str, Any]]) -> str:
        """Format execution trace for LLM analysis"""
        if not execution_trace:
            return "No execution trace available"
        
        formatted_lines = []
        for i, trace in enumerate(execution_trace[:100]):
            method = trace.get("method", "Unknown")
            file_location = trace.get("file", "unknown")
            line = trace.get("line", 0)
            source = trace.get("source", "").strip()
            context = trace.get("context", "")
            
            formatted_lines.append(f"""
{i+1}. **{method}** ({file_location}:{line})
   ```
   {source}
   ```
   Context: {context}
""")
        
        if len(execution_trace) > 15:
            formatted_lines.append(f"\n... and {len(execution_trace) - 15} more method calls")
        
        return "\n".join(formatted_lines)
    
    def _parse_vulnerability_response(self, response_content: str) -> List[Dict[str, Any]]:
        """Parse LLM response to extract vulnerabilities"""
        try:
            logger.debug(f"🔍 RAW LLM RESPONSE (first 500 chars): {response_content[:500]}")
            
            # Try multiple strategies to extract JSON
            json_content = None
            
            # Strategy 1: Look for ```json code blocks
            if '```json' in response_content:
                start_marker = response_content.find('```json') + 7
                end_marker = response_content.find('```', start_marker)
                if end_marker != -1:
                    json_content = response_content[start_marker:end_marker].strip()
                    logger.debug("📋 Found JSON in code block")
            
            # Strategy 2: Look for JSON object boundaries
            if not json_content:
                start_idx = response_content.find('{')
                if start_idx != -1:
                    # Count braces to find matching closing brace
                    brace_count = 0
                    end_idx = start_idx
                    for i, char in enumerate(response_content[start_idx:], start_idx):
                        if char == '{':
                            brace_count += 1
                        elif char == '}':
                            brace_count -= 1
                            if brace_count == 0:
                                end_idx = i + 1
                                break
                    
                    if brace_count == 0:
                        json_content = response_content[start_idx:end_idx]
                        logger.debug("📋 Found JSON by brace matching")
            
            # Strategy 3: Try to find just vulnerabilities array
            if not json_content:
                vuln_match = re.search(r'"vulnerabilities"\s*:\s*\[([^\]]*(?:\[[^\]]*\][^\]]*)*)\]', response_content, re.DOTALL)
                if vuln_match:
                    vuln_array = vuln_match.group(1).strip()
                    json_content = f'{{"vulnerabilities": [{vuln_array}]}}'
                    logger.debug("📋 Found vulnerabilities array only")
            
            if not json_content:
                logger.warning("❌ No JSON content found in LLM response")
                return []
            
            logger.debug(f"🔧 JSON CONTENT (first 300 chars): {json_content[:300]}")
            
            # Try to parse directly first
            try:
                parsed = json.loads(json_content)
                logger.debug("✅ Direct JSON parsing successful")
            except json.JSONDecodeError:
                logger.debug("🔧 Direct parsing failed, applying fixes...")
                # Apply fixes and try again
                json_content = self._fix_json_content(json_content)
                logger.debug(f"🔧 FIXED JSON CONTENT (first 300 chars): {json_content[:300]}")
                parsed = json.loads(json_content)
                logger.debug("✅ Fixed JSON parsing successful")
            
            vulnerabilities = parsed.get("vulnerabilities", [])
            
            # Store vuln-chaser analysis for later use
            vuln_chaser_analysis = parsed.get("vuln_chaser_analysis", {})
            
            # Validate and clean up vulnerabilities
            valid_vulnerabilities = []
            for vuln in vulnerabilities:
                if self._validate_vulnerability(vuln):
                    # Add vuln-chaser analysis context to each vulnerability
                    vuln['vuln_chaser_context'] = vuln_chaser_analysis
                    valid_vulnerabilities.append(vuln)
                else:
                    logger.warning(f"Invalid vulnerability format: {vuln}")
            
            logger.info(f"✅ Successfully parsed {len(valid_vulnerabilities)} vulnerabilities")
            return valid_vulnerabilities
            
        except json.JSONDecodeError as e:
            logger.error(f"❌ Failed to parse JSON from LLM response: {e}")
            logger.error(f"📋 Raw response: {response_content}")
            if 'json_content' in locals():
                logger.error(f"🔧 Processed JSON: {json_content}")
            
            # Final fallback: return empty list
            logger.warning("🔄 Returning empty vulnerabilities list as fallback")
            return []
            
        except Exception as e:
            logger.error(f"❌ Unexpected error parsing vulnerability response: {e}")
            return []
    
    def _validate_vulnerability(self, vuln: Dict[str, Any]) -> bool:
        """Validate vulnerability structure"""
        required_fields = ["owasp_category", "type", "severity", "confidence"]
        
        for field in required_fields:
            if field not in vuln:
                logger.warning(f"Missing required field: {field}")
                return False
        
        # Validate OWASP category
        valid_categories = ["A01:2021", "A02:2021", "A03:2021", "A05:2021", "A06:2021", "A07:2021"]
        if vuln["owasp_category"] not in valid_categories:
            logger.warning(f"Invalid OWASP category: {vuln['owasp_category']}")
            return False
        
        # Validate severity
        valid_severities = ["low", "medium", "high", "critical"]
        if vuln["severity"] not in valid_severities:
            logger.warning(f"Invalid severity: {vuln['severity']}")
            return False
        
        # Validate confidence
        try:
            confidence = float(vuln["confidence"])
            if not (0.0 <= confidence <= 1.0):
                logger.warning(f"Invalid confidence value: {confidence}")
                return False
        except (ValueError, TypeError):
            logger.warning(f"Invalid confidence type: {vuln['confidence']}")
            return False
        
        return True
    
    def _fix_json_content(self, json_content: str) -> str:
        """Fix common JSON formatting issues in LLM responses"""
        
        # Remove markdown code block markers
        json_content = json_content.replace('```json', '').replace('```', '')
        json_content = json_content.strip()
        
        # Fix trailing commas before closing braces/brackets  
        json_content = re.sub(r',(\s*[}\]])', r'\1', json_content)
        
        # Fix unescaped newlines and tabs in strings
        json_content = json_content.replace('\n', '\\n').replace('\t', '\\t')
        
        # Fix unescaped quotes in string values (simple approach)
        # This regex looks for quotes that are likely inside string values
        json_content = re.sub(r':\s*"([^"]*)"([^,}\]]*)"([^"]*)"', r': "\1\\\"\2\\\"\3"', json_content)
        
        # Ensure property names are quoted
        json_content = re.sub(r'([{,]\s*)([a-zA-Z_][a-zA-Z0-9_]*)(\s*):', r'\1"\2"\3:', json_content)
        
        return json_content
    
    def _format_sor_context(self, subject: Dict[str, Any], operations: List[Dict[str, Any]], 
                           resources: List[Dict[str, Any]], relationships: Dict[str, Any], 
                           summary: Dict[str, Any]) -> str:
        """Format SOR analysis results for prompt inclusion"""
        
        context_parts = []
        
        # Subject Analysis
        context_parts.append("### Subject Analysis")
        context_parts.append(f"- Type: {subject.get('type', 'unknown')}")
        context_parts.append(f"- Authentication Status: {subject.get('authentication_status', 'unknown')}")
        context_parts.append(f"- Privilege Level: {subject.get('privilege_level', 'unknown')}")
        context_parts.append(f"- Trust Boundary: {subject.get('trust_boundary', 'unknown')}")
        
        # Operations Analysis
        if operations:
            context_parts.append("\n### Operations Analysis")
            for i, op in enumerate(operations[:5]):  # Limit to first 5 operations
                primary = op.get('primary_classification', {})
                context_parts.append(f"**Operation {i+1}**: {op.get('trace_method', 'unknown')}")
                context_parts.append(f"  - Category: {primary.get('category', 'unknown')}")
                context_parts.append(f"  - Risk Level: {primary.get('risk_level', 'unknown')}")
                context_parts.append(f"  - Implementation Quality: {primary.get('implementation_quality', 'unknown')}")
                vuln_patterns = op.get('vulnerability_patterns', [])
                if vuln_patterns:
                    context_parts.append(f"  - Vulnerability Patterns: {', '.join(vuln_patterns)}")
        
        # Resources Analysis
        if resources:
            context_parts.append("\n### Resources Analysis")
            for i, res in enumerate(resources[:5]):  # Limit to first 5 resources
                context_parts.append(f"**Resource {i+1}**: {res.get('trace_method', 'unknown')}")
                context_parts.append(f"  - Type: {res.get('type', 'unknown')}")
                context_parts.append(f"  - Sensitivity: {res.get('sensitivity', 'unknown')}")
                context_parts.append(f"  - Access Pattern: {res.get('access_pattern', 'unknown')}")
                protections = res.get('protection_mechanisms', [])
                context_parts.append(f"  - Protection Mechanisms: {', '.join(protections) if protections else 'None'}")
        
        # Relationship Analysis
        context_parts.append("\n### SOR Relationship Analysis")
        violations = relationships.get('violations', [])
        if violations:
            context_parts.append(f"**Detected Violations ({len(violations)} total):**")
            for violation in violations[:5]:  # Limit to first 5 violations
                v_type = violation.get('type', 'unknown')
                severity = violation.get('severity', 'unknown')
                desc = violation.get('description', '')
                context_parts.append(f"  - {v_type} (Severity: {severity}): {desc}")
        else:
            context_parts.append("**Detected Violations**: None")
        
        risk_score = relationships.get('risk_score', 0)
        context_parts.append(f"**Risk Score**: {risk_score}/10.0")
        
        recommendations = relationships.get('recommendations', [])
        if recommendations:
            context_parts.append(f"**Recommendations**: {', '.join(recommendations[:3])}")
        
        # Summary
        if summary:
            context_parts.append("\n### Analysis Summary")
            context_parts.append(f"- Overall Risk Level: {summary.get('risk_level', 'unknown')}")
            context_parts.append(f"- Total Violations: {summary.get('total_violations', 0)}")
            context_parts.append(f"- Key Finding: {summary.get('message', 'None')}")
        
        return "\n".join(context_parts)
    
    def _enhance_vulnerabilities_with_sor(self, vulnerabilities: List[Dict[str, Any]], 
                                         sor_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Enhance detected vulnerabilities with SOR context information"""
        
        enhanced_vulnerabilities = []
        relationships = sor_analysis.get('relationships', {})
        
        for vuln in vulnerabilities:
            # Create enhanced copy
            enhanced_vuln = vuln.copy()
            
            # Add SOR context enhancement flag
            enhanced_vuln['sor_context_enhanced'] = True
            
            # Add related SOR violations
            violations = relationships.get('violations', [])
            related_violations = []
            for violation in violations:
                # Check if violation is related to this vulnerability type
                if self._is_violation_related_to_vulnerability(violation, vuln):
                    related_violations.append({
                        'type': violation.get('type', 'unknown'),
                        'severity': violation.get('severity', 'unknown'),
                        'description': violation.get('description', '')
                    })
            
            enhanced_vuln['related_sor_violations'] = related_violations
            
            # Add risk amplification information based on SOR analysis
            risk_score = relationships.get('risk_score', 0)
            if risk_score >= 7.0:
                enhanced_vuln['sor_risk_amplification'] = "High risk confirmed by SOR analysis"
                # Increase severity if supported by high SOR risk
                if enhanced_vuln['severity'] in ['low', 'medium'] and related_violations:
                    enhanced_vuln['severity'] = 'high'
                    enhanced_vuln['severity_boost_reason'] = 'SOR violation support'
            elif risk_score >= 4.0:
                enhanced_vuln['sor_risk_amplification'] = "Medium risk confirmed by SOR analysis"
            else:
                enhanced_vuln['sor_risk_amplification'] = "Low risk per SOR analysis"
            
            # Adjust confidence based on SOR analysis support
            if related_violations:
                # Increase confidence if SOR violations support the vulnerability
                original_confidence = float(enhanced_vuln.get('confidence', 0.5))
                boost = min(0.3, len(related_violations) * 0.1)  # Max boost of 0.3
                enhanced_vuln['confidence'] = min(1.0, original_confidence + boost)
                enhanced_vuln['confidence_boost_reason'] = f"Supported by {len(related_violations)} SOR violation(s)"
            
            # Add SOR-based remediation context
            if 'recommendation' in enhanced_vuln:
                sor_recommendations = relationships.get('recommendations', [])
                if sor_recommendations:
                    enhanced_vuln['sor_remediation_context'] = f"SOR Analysis suggests: {', '.join(sor_recommendations[:2])}"
            
            enhanced_vulnerabilities.append(enhanced_vuln)
        
        return enhanced_vulnerabilities
    
    def _is_violation_related_to_vulnerability(self, violation: Dict[str, Any], 
                                             vulnerability: Dict[str, Any]) -> bool:
        """Check if a SOR violation is related to a detected vulnerability"""
        
        violation_type = violation.get('type', '').lower()
        vuln_type = vulnerability.get('type', '').lower()
        vuln_category = vulnerability.get('owasp_category', '')
        
        # Define mapping between SOR violations and vulnerability types/categories
        violation_mappings = {
            'trust_boundary_violation': {
                'vuln_types': ['injection', 'sql_injection', 'command_injection', 'xss', 'xxe'],
                'owasp_categories': ['A03:2021']  # Injection
            },
            'privilege_escalation': {
                'vuln_types': ['access_control', 'authorization', 'authentication', 'privilege'],
                'owasp_categories': ['A01:2021', 'A07:2021']  # Access Control, Authentication
            },
            'insufficient_validation': {
                'vuln_types': ['injection', 'sql_injection', 'xss', 'validation', 'sanitization'],
                'owasp_categories': ['A03:2021', 'A05:2021']  # Injection, Security Misconfiguration
            },
            'direct_access': {
                'vuln_types': ['access_control', 'authorization', 'path_traversal', 'file_access'],
                'owasp_categories': ['A01:2021', 'A05:2021']  # Access Control, Security Misconfiguration
            }
        }
        
        # Check if violation type maps to this vulnerability
        mapping = violation_mappings.get(violation_type, {})
        
        # Check vulnerability type matching
        related_types = mapping.get('vuln_types', [])
        if any(vtype in vuln_type for vtype in related_types):
            return True
        
        # Check OWASP category matching
        related_categories = mapping.get('owasp_categories', [])
        if vuln_category in related_categories:
            return True
        
        return False
    
    def _compute_trace_hash(self, execution_trace: List[Dict[str, Any]]) -> str:
        """Compute hash of execution trace for deduplication based on call chain pattern"""
        if not execution_trace:
            return "empty_trace"
        
        # Extract key components for hashing - focus on structural patterns, not specific values
        hash_components = []
        for trace in execution_trace:
            # Use method name and file path as primary identifiers
            method = trace.get('method', '')
            file_path = trace.get('file', '')
            
            # Extract security-relevant context patterns (not specific values)
            context = trace.get('context', '')
            risk_level = trace.get('risk_level', '')
            param_usage = trace.get('parameter_usage', {})
            
            # Create normalized patterns for security analysis
            security_pattern = []
            if param_usage.get('uses_request_params'):
                security_pattern.append('uses_params')
            if param_usage.get('direct_interpolation'):
                security_pattern.append('direct_interpolation')
            if param_usage.get('sanitization_detected'):
                security_pattern.append('sanitized')
            
            # Create structural signature (excluding variable content like line numbers, specific source)
            signature = f"{method}:{file_path}:{context}:{risk_level}:{':'.join(sorted(security_pattern))}"
            hash_components.append(signature)
        
        # Create a deterministic hash of the call chain pattern
        trace_pattern = "|".join(hash_components)
        return hashlib.sha256(trace_pattern.encode('utf-8')).hexdigest()
    
    def clear_analysis_cache(self):
        """Clear the analysis cache"""
        cache_size = len(self.analysis_cache)
        self.analysis_cache.clear()
        self.cache_hits = 0
        self.cache_misses = 0
        logger.info(f"🧹 Cleared analysis cache ({cache_size} entries)")
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """Get cache performance statistics"""
        total_requests = self.cache_hits + self.cache_misses
        hit_rate = (self.cache_hits / total_requests * 100) if total_requests > 0 else 0
        
        return {
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses,
            'total_requests': total_requests,
            'hit_rate_percent': round(hit_rate, 2),
            'cache_size': len(self.analysis_cache)
        }
    
    def get_sor_performance_metrics(self) -> Dict[str, Any]:
        """Get SOR Framework performance metrics"""
        base_metrics = {
            'total_sor_enhanced_analyses': self.sor_enhanced_count,
            'last_analysis_time_ms': self.last_analysis_time,
            'last_cost_usd': self.last_cost
        }
        
        # Add cache performance metrics
        cache_stats = self.get_cache_stats()
        base_metrics['cache_performance'] = cache_stats
        
        # Get SOR engine performance metrics if available
        if hasattr(self.sor_engine, 'get_performance_metrics'):
            sor_metrics = self.sor_engine.get_performance_metrics()
            base_metrics['sor_component_metrics'] = sor_metrics
        
        return base_metrics
